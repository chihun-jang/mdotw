---
title: '신경망 구성'
date: '2020-06-11'
category: ['딥러닝', '머신러닝']
draft: False
---

## 신경망

퍼셉트론으로는 복잡한 함수를 표현할수 있지만 여전히 가중치는 사람이 수동으로 해야한다.
신경망은 퍼셉트론의 이러한 단점(수동으로 가중치를 설정하는 부분)을 해결해준다.

즉. **신경망은 가중치 매개변수의 적절한 값을 data로 부터 자동으로 학습하는 능력이 있다**.

신경망은 **입력층, 은닉층, 출력층**으로 구성되어있다. (이때 은닉층의 뉴런은 사람의 눈에는 보이지 않아서 은닉층이다.)

퍼셉트론에서는 우리가 x1,x2노드가 한개의 node로 입력이 들어가고 판단하는 모양이었는데 계산 식이 `x1*w1+ x2*w2 + b`로 바뀐만큼

입력하는 노드를 `x1,x2`가 아닌 `x1,x2,b`로 추가해주고 b의 입력은 항상 값이 1이고 가중치를 b로 설정해주도록 하자.
그리고 조금 더 노드를 상세하게 표현해 **활성화 함수**라는것을 명시적으로 나타내 주게 되는데 이부분이 퍼셉트론과 신경망이 구분되는 가장 큰 특징이다.

> 활성화 함수  
> 활성화 함수는 다름이 아니라 x1,x2의 입력을 받은 node가 입력값들의 총합을 0보다 큰지,작은지 판단해서 출력을 결정짓게 되는데 이 판단과 결정을 하는 부분을 **활성화 함수**라고 한다.
> (일반적으로는 노드안에 숨겨놓고 안보여 주지만 상세하게 설명을 하려 한다면 입력을 받는 노드에서 상세히 활성화함수를 상세히 기술해주기도한다.)
> **단층퍼셉트론은 계단함수를 활성화 함수**로 사용하고 **다층퍼셉트론은 시그모이드 함수와 같은 매끈한 모양의 함수를 활성화 함수**로 사용한다.

우리가 지금까지 사용했던 **임계값을 기점으로 0,1 이렇게 출력이 바뀌는 것은 계단함수**라고 한다.(그리고 **이것이 퍼셉트론**이다)
반면 **활성화 함수로 계단함수가 아닌 다름 함수를 사용하는 것이 신경망**이다.

## 시그모이드 함수

시그모이드 함수는`h(x) = 1/(1+exp(-x))`의 모양을 가진 함수이다.
신경망에서는 **활성화 함수로 시그모이드 함수를 이용**하고 변환된 신호를 다음 뉴런으로 전달하게된다.

시그모이드 함수는 **특정 임계점을 경계로 0,1이렇게 변화시키는 것이 아니라 매끄러운 값을 출력해주므로 신경망학습에 좋다.**
입력값이 중요하면 1에 가까운 값을 출력하고 입력값이 덜 중요하면 0에 가깝게 처리해준다.

시그모이드함수는 비선형함수인데 신경망에서는 활성화함수로 비선형함수를 사용해야한다.
왜냐면 활성화함수로 선형을 사용하면 신경망의 층을 깊게하는 의미가 없어진다.(계수에 연산이 아무리 들어가봤자 임의의 상수로 치환하여 표현가능하다.)
(퍼셉트론도 활성화함수는 계단함수를 사용한다(비선형함수))

## ReLU함수

최근 많이 사용되고있는 활성화 함수로 입력이 0을 초과면 그대로 출력하고 0 이하면 0을 출력하는 함수이다.
(ReLU의 기원은 정류된이라는 의미로 0이하면 -를 차단하는 의미로 ReLU함수라고 부르게 됐다.)

## 뉴런의 입력값 연산하기

입력값 x와 b 그리고 가중치 w를 계산해보면 `A = XW + B` 이렇게 간소화 할수 있다.
그리고 행렬간의 계산은 모양을 맞춰줘야지 된다.

이렇게 0층,1층,2층 이런식으로 진행되는 것을 순방향이라고 하고 우리는 앞으로 역방향으로도 진행시켜볼거다.

## 출력층 설계하기

신경망은 **분류(Discrete)**와 **회귀(Continuous)** 모두 이용할수있다.
일반적으로 **회귀에는 항등함수**를 **분류에는 소프트맥스함수**를 사용한다.

-   항등함수  
    항등함수의 경우 입력값이 그냥 출력값이 되므로 그냥 구현하면 된다.

-   소프트맥스  
    소프트맥스 함수의 경우 확률로 나타내지는데 `특정출력뉴련/ 모든출력뉴런`의 모양으로 나타낸다. 따라서 각 출력층은 모든 입력신호로부터 영향을 받는다.

소프트맥스 함수를 구현할때는 조심해야 할 부분이 **overflow**인데 그 이유는 지수함수로 이루어져 있는만큼 값의 증가가 급속도로 이루어지기 때문이다.
따라서 식의 변형을 거쳐 정규화를 해줘야하는데 일반적으로 입력값중 최댓값을 이용한다.(최댓값을 입력값에서 빼주어서 정규화를 해준다.)

이때 소프트맥스를 사용한 **출력값의 총합은 1이 되는데 이는 확률로 해석할수 있게 한다**.

> 소프트맥스 함수는 단조증가 함수이므로 각 입력 data간의 크기순서가 변하지 않는데 따라서 현업에서는 함수 계산에 드는 자원낭비를 줄이고자 추론단계(test)에서의 소프트맥스함수는 생략한다. 대신 학습(train)단계에서의 소프트 맥스함수는 사용해야한다.

### 출력층의 뉴런수 정하기

문제에 맞게끔 출력 뉴런수를 정하면 되는데 일반적으로 분류하고 싶은 클래스의 수로 설정한다.

ex) 숫자를 분류하는 문제 ==> 0~9 10개의 뉴런을 출력

(이와같이 특정 처리를 가하는 것을 전처리라고 한다)

-   `normalize` : 정규화(여기서는 0.0~1.0)사이의 값으로 하는 여부
-   `flatten` : 입력 받은 값을 1차원 배열로 변환하자.
-   `one-hot` : [0,0,1,0,0,0] 과 같은 방식으로 정답인 원소만 1로 만들어주고 나머지는 0으로 처리하는 방법.
-   `argmax` : 배열중에 가장 값이 큰 원소의 인덱스를 구한다 ==> 해당 숫자가 될 확률이 가장 높다는 뜻이다.

> **Pickle**
> 프로그램 실행중 특정 객체를 파일로 저장하는 기능으로 저장해둔 pickle파일을 로드하면 실행 당시의 객체를 즉시 복원할수 있다. 마치 캐싱과 같은 기능

이미지를 flatten으로 1차원 배열로 만든경우 다시 이미지를 표시할때 reshape()로 원래 shape로 돌아가서 그려줘야한다.

> 현업에서도 신경망(딥러닝)에 전처리를 활발히 사용 ==> 전처리를 사용해서 능력개선 및 학습속도 향상을 한다. 데이터 백색화 및 평균과 표준편차를 이용하는 방법등 다양한 방법으로 정규화같은 전처리를 해줄수 있다

## 배치(batch)

입력데이터를 하나씩 입력하는게 아니라 묶음 단위로 넣어줄수도 있는데 이를 **batch(배치)**라고 한다.

-   수치계산 라이브러리들이 큰 배열을 처리하는데 효과적으로 구성되어 있고
-   무엇보다도 매번 데이터를 읽어오게 되면 **I/O때문에 CPU사용량이 많아져서** 시간이 오래걸리므로 한번에 많은 DATA를 읽어 부하를 줄여준다.

### 👨‍💻 실습코드

```python
# 계단함수 구현하기

import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    if x > 0:
        return 1
    else:
        return 0

# 위의 함수는 실수는 받아들일 수 있지만 배열을 arg로 받을 수는 없다.
# 따라서 배열을 받을수있게 수정해준다면

def step_function2(x):
    y = x > 0
    return y.astype(np.int)

# 위와같이 처리해주면 배열의 각각의 원소를 부등호로 판별해 배열을 리턴하는데
# 그 리턴된 배열의 True, false값을 int로 변환해서 return하는 모양이다.

def step_function3(x):
    return np.array(x > 0, dtype=np.int)

def sigmoid(x):
    return 1 / (1+np.exp(-x))

# x = np.arange(-5.0, 5.0, 0.1)
# y = step_function3(x)
# y2 = sigmoid(x)
# plt.plot(x, y)
# plt.plot(x, y2,  linestyle="--")

# plt.ylim(-0.1, 1.1)
# plt.show()

# ReLU함수 구현하기

def relu(x):
    return np.maximum(0, x)

A = np.array([1, 2, 3, 4])
print(A)
print(np.ndim(A))  # ndim으로 배열의 차원수 확인가능하고
print(A.shape)  # shape로 모양을 알수있고,(그런데 1차원 배열도 튜플로 반환하는 이유는 다차원 배열과 일치시켜주기 위해)
print(A.shape[0])

print("*"*50)
X = np.array([1, 2])
print(X.shape)
W = np.array([[1, 3, 5], [2, 4, 6]])
print(W.shape)
Y = np.dot(X, W)
print(Y)

# 이처럼 행렬을 계산하는데 원소의 갯수와 상관없이 dot연산자를 쓰면 된다.

print("*"*50)

X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1) + B1
Z1 = sigmoid(A1)
print(A1)
print("시그모이드 처리:", Z1)

W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

# 마지막 출력층 구현하기 (다 비슷하지만 활성화 함수만 다름)
# 이때 출력층의 활성화함수는 문제에 맞춰 다르게 설정하게 되고, (회귀 문제는 항등함수, 이진 분류는 시그모이드, 다중분류는 softmax분류를 사용한다.)

def identity_function(x):
    return x
W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)
print(Y)

# softmax함수 구현하기
print("#"*100)
a = np.array([0.3, 2.9, 4.0])
exp_a = np.exp(a)
print(exp_a)
sum_exp_a = np.sum(exp_a)
print(sum_exp_a)

y = exp_a/sum_exp_a
print(y)


def softmax(a):
    exp_a = np_exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
# softmax함수를 구현할때오버플로가 일어나는 부분을 조심해야하는데 추가 처리를 해줘야한다

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a-c)  # 오버플로 대책
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

a = np.array([0.3, 2.9, 4.0])

y = softmax(a)
print(y)
print(np.sum(y))

```
