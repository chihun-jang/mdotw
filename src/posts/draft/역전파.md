---
title: '오차역전파법'
date: '2020-06-21'
category: []
draft: True
---

4장에서는 수치미분을 이용해서 구현을 했는데 **수치미분은 단순하고 구현이 쉽지만 시간이 오래** 걸린다. 따라서 **오차역전파법(역전파)**(`backpropagation`) 을 이용해서 구현해보도록하자

### 👈 backpropagation

수식을 통한 이해와 계산 그래프를 통한 이해가 있다.

### 계산그래프

계산과정을 **_그래프 자료구조_**로 나타낸것으로 `node` 와 `edge` 로 표현한다.

> 사과 --100원--> X2(노드) --200--> x1.1(노드) --220-->

이처럼 **node안에 연산내용**이 들어가고 **edge를 통해서 data**가 움직인다.

이때 그래프에서 계산을 왼쪽에서 오른쪽으로 진행하는 것을 순전파라 한다.
반대방향으로는 역전파라하고 미분을 계산할때 중요한 역할을 한다.

계산그래프는 **_국소적 계산을 통해서 최종결과를 도출_**해 낸다.
각 노드는 자신과 연관된 계산 외에는 신경쓸 필요가 없다.

-   국소적 계산을 통해 각 문제에 집중함으로 단순화시킬수있다.
-   중간 계산 결과를 모두 보관할수 있다.
-   역전파를 통해 미분을 효율적으로 계산할수 있다.

순전파의 반대 화살표를 굵은선으로 그리고 아래에 미분값을 적어 비(ratio)를 볼수있다.

### 연쇄법칙

역전파가 국소 미분을 전달하는 원리는 `chain Rule`에 따른것이다.

합성함수 : 여러함수로 분리해 나타낼수있는 함수

```python
z = (x+y)^2
#위의 식은 아래와 같이 분리할수있다.
z = t^2
t = x+y
```

합성함수의 미분은 **_합성함수를 구성하는 각 함수의 미분의 곱_**으로 나타낼수있다.(미분표기의 곱셈으로 나타내줄수 있다는 말)`dt/dy*dy/dx`

연쇄법칙은 어렵게 생각할 필요없이 내가 필요한 함수를 분할해서 여러 함수의 곱으로 나타낼수 있는것이다.

---

### 역전파

덧셈의 역전파는 그대로 전달해주기때문에 순전파의 입력신호가 필요없었는데
역전파는 **_다른 입력신호의 값을 곱해야하므로 순방향 입력신호의 값이 필요해서 저장_** 해둔다.

### 단순한 계층 구현하기

다음 절에서는 **_신경망을 구성하는 계층(계층이랑 신경망의 기능단위)을 class로 구현한다._**

모든 계층은 `forward()`(순전파)와 `backward()`(역전파)라는 공통의 method, 인터페이스를 갖도록 구현할것이다.

-   `곱셈계층` : 곱셈계층은 역전파로 **_돌아갈때 입력신호의 상대값이 필요_**하므로 저장해놓는다.
-   `덧셈계층` : 덧셈계층은 역전파로 돌아갈때 그냥 보내줘도 되니까 init으로 x와 y를 따로 초기화해서 저장해주지 않는다.

위와 같은 방법으로 계산그래프를 이용하면 복잡한 미분도 손쉽게 할수 있다.
(계산그래프에서 연쇄법칙을 이용하여 역전파를 구해놨으면 해당 data가 출력값에 얼마만큼의 영향을 미치는지 확인해줄 수 있다.==>즉 미분과 같이 변화율에 대해서 구할 수 있다.)

### 활성화 함수 계층 구현하기

신경망을 구성하는 계층을 각각 하나의 클래스로 구현한다.

### ReLU 계층 구현하기

-   ReLU의 수식

```
y = x (x>0)
y = 0 (x<=0)
```

-   ReLU의 미분식

```
dy/dx = 1 (x>0)
dy/dv = 0 (x<=0)
```

> 따라서 위의 식을 보면 순전파의 입력이 0이하면 역전파로 돌아갈때 하류로 신호를 보내지 않는다. 입력이 0이상이면 그대로 전해준다.

```python
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        # x는 넘파이 배열로 들어오는데 x가 0보다 작거나 같은 index를 True로 하여 배열생성
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0  # 0보다 작거나 같은 애들이 True이므로 그 index에 해당하는 ele를 0으로 바꿈

        return out

    # 역전파는 순전파때 만든 mask를 이용해서 True인 index에 대해서 dout을 0으로 전달한다.
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

### sigmoid

```
y = 1 / (1+exp(-x))
```

연산 노드의 순서를 보면

`x --> exp --> + --> /` 식으로 나뉘어진다.
역전파 순서대로 오른쪽 --> 왼쪽으로 역행하면서 sigmoid의 역전파를 한번 구현해보자

1. `/` 노드 미분( y = 1/x )
   `dy/dx = -(1/x^2) = -y^2`

따라서 해당 노드를 지나면 상류에서 흘러들어온 값에 `-y^2`을 곱해서 하류로 보낸다

2. `+` 노드
   별다른 작업없이 하류로 보낸다(그대로 전달)

3. `exp` 노드(y=e^x)
   `dy/dx = e^x`

따라서 1에서 곱해진 값에 더해서`e^x`를 곱해서 전해준다.

4.  `x` 노드
    순전파 때 입력받은 값을 서로 바꿔 곱하므로
    여기서는 -1이 순전파때 다른 입력으로 주어졌으므로 `-1`을 곱하면 된다.

5.  최종출력
    `dL/dy(y^2*e^-x)`

    입력값인 x와 출력값인 y만으로 계산이 되고 중간과정을 모두 묶어 `sigmoid node`라는 역전파 노드 하나로 대체해버릴수 있다.

    (이럴때 중간과정을 생략하여 보다 효율적이기도 하지만 그룹화 해서 외부에 내용을 노출하지 않는것도 중요포인트로 작용한다.)

    `dL/dy(y^2*e^-x)` = `dL/dy(y*(1-y))`

    위의 식으로 바꿔서 계산할수 있는데 이는 **출력(y)**만으로도 계산할수 있다는 말이 된다. 그리고 이를 이용해서 `backward`를 구현한다.

#### Affine/Softmax 계층 구현하기

신경망의 순전파에서 수행하는 행렬의 곱을 기하학에서 `affine transformation`이라고 한다.

따라서 Affien변환을 수행하는 처리를 Affine계층으로 구현한다.

이때 역전파를 구할 때도 행렬에 대응하는 **차원의 원소수가 일치하도록** 곱을 조립하여 구한다.

#### Softmax - with - Loss 계층

출력층에서 사용하는`softmax`함수에 대해서 알아보자
softmax는 입력값을 정규화(이는 총합이 1로 확률로 나타난다)해서 출력한다.

> 신경망을 학습시킬때는 **_softmax함수로 정규화_** 해줘야하는데, 신경망을 이용해 추론하는 과정에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되므로 **_굳이 softmax함수를 쓸 필요는 없다_**. 왜냐하면 sorting된 순서는 같게 움직이기 때문이다.

`softmax` 계층의 역전파는 `(y1-t1, y2-t2)`와 같은 방식으로 말끔한 결과를 보여주는데 이때의 y는 softmax계층의 출력이고 t는 정답 레이블이므로 출력과 정답의 **차분이 앞계층으로 전해지는 것**이다.

신경망의 목적은 **신경망의 출력(softmax의 출력)**이 정답과 가까워지도록 가중치 매개변수를 조정하는것이다. 따라서 효율적으로 앞으로 전달해야한다.

역전파로 위와같은 모양이 나오는 이유는 교차 엔트로피 오차함수가 편하게 전해주도록 설계가 되어있기때문이다.
이와 같은 원리로 항등함수의 손실함수로 오차제곱합을 하는 이유도 말끔히 떨어지게 설계해놔서이다.

### 오차역전파법(backpropagation) 구현

이러한 계층들을 조합하면 레고조립하듯 신경망을 구축할수있다. 오차역전파법이 등장하는 단계는 기울기 산출단계에서 적용해줄수있는데
기존의 수치미분에 비해서 **_효율적이고 빠르게 연산_**을 해줄수있다.

이번 장에서 각 계층을 모듈화해서 만들어놨기때문에 여러층이더라도 효율적으로 구현할수 있다.

### 기울기 검증하기

1.  수치미분을 써서 구하는 방법
2.  해석학적 방법(오차역전파법)

변화율을 구하는 방법에는 1번과 2번의 방법이 있는데 2번을 구현해놓으면 1번은 성능상 쓸모가 없을지도 모르지만 **_1번방법이 구현이 쉬우므로 우리는 1번을 통해서 2번을 검증_**하도록 한다.

### 정리

계산그래프의 **_순전파는 통상의 계산_**을 하고
계산그래프의 **_역전파는 각 노드의 미분_**을 구할수 있다.

### 👨‍💻실습코드

```python
class MulLayer:
    # 객체변수인 x와 y를 초기화한다.
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x*y
        return out

    def backward(self, dout):
        dx = dout*self.y
        dy = dout * self.x
        return dx, dy

# 사과 예제를 MulLayer를 사용하여 풀어보기


# apple = 100
# apple_num = 2
# tax = 1.1

# mul_apple_layer = MulLayer()
# mul_tax_layer = MulLayer()
# apple_price = mul_apple_layer.forward(apple, apple_num)
# price = mul_tax_layer.forward(apple_price, tax)

# print(price)

# # 각 변수에 대한 미분은 backward에서 구하자

# dprice = 1
# dapple_price, dtax = mul_tax_layer.backward(dprice)
# dapple, dapple_num = mul_apple_layer.backward(dapple_price)

# print(dapple, dapple_num, dtax)
# backward()가 받는 인수는 순전파의 출력에 대한 미분


class AddLayer:
    # 덧셈계층은 x, y가 굳이 필요없으니 init이 아무일도 하지않는다
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x+y
        return out

    # backward() 에서는 출력으로부터 돌아오는 값을 그래도 다시 흘려보낸다
    # 상류 -> 하류로
    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy


apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

# 계층들
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

apple_price = mul_apple_layer.forward(apple, apple_num)
orange_price = mul_orange_layer.forward(orange, orange_num)

all_price = add_apple_orange_layer.forward(apple_price, orange_price)
price = mul_tax_layer.forward(all_price, tax)

dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)
dorange, dorange_num = mul_orange_layer.backward(dorange_price)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print(price)
print(dapple_num, dapple, dorange, dorange_num, dtax)


class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        # x는 넘파이 배열로 들어오는데 x가 0보다 작거나 같은 index를 True로 하여 배열생성
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0  # 0보다 작거나 같은 애들이 True이므로 그 index에 해당하는 ele를 0으로 바꿈

        return out

    # 역전파는 순전파때 만든 mask를 이용해서 True인 index에 대해서 dout을 0으로 전달한다.
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx

# ReLU는 전형적인 전기회로 문제와 비슷하다 전류가 흐를때는 전류를 흘려보내고
# 스위치가 꺼지면 전류가 아예 안흘러가는


class Sigmoid:
    def __init__(self):
        self.out = None

    # 순전파의 출력을 out에 보관하다가 역전파 계산때 값을 사용한다.
    def forward(self, x):
        out = 1/(1+np.exp(-x))
        self.out = out

        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out)*self.out

        return dx


class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)

        return dx


class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None  # 손실
        self.y = None  # softmax의 출력
        self.t = None  # 정답레이블(원핫인코딩)

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]

        # 역전파로 전파하는 값을 배치의 수로 나눠 data1개당 오차를 앞으로 전파
        dx = (self.y - self.t) / batch_size

        return dx

```
